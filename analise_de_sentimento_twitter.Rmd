---
title: "R - Analise de sentimento baseado em tweets"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---
## Analise de sentimento baseado em Tweets
Análise de sentimento desenvolvido em [R Markdown](http://rmarkdown.rstudio.com) Notebook.

Neste projeto, foi realizado a análise de tweets sobre a cooperativa de crédito Sicredi. O Sicredi é uma instituição financeira cooperativa, além de ser referência internacional pelo modelo de atuação, com cerca de 1600 agências espalhadas pelo Brasil. Para conhecer mais sobre a empresa, clique [aqui.](https://www.sicredi.com.br/html/conheca-o-sicredi/quem-somos/?utm_source=menu_topo&utm_medium=topo_site&utm_campaign=quem_somos)
Este é um projeto particular utilizando dados públicos e não reflete qualquer opnião pessoal sobre a instituição.
No projeto, utilizamos a linguagem R versão 3.5.0. Foram utilizadas várias bibliotecas descritas no decorrer o projeto, e como menciona o título do projeto, foi utilizado a rede social Twitter como base na análise.
--
##Bibliotecas utilizadas
Importando bibliotecas necessárias
```{r}
require(twitteR)
require(wordcloud)
require(igraph)
library(twitteR)
library(tidyverse)
library(data.table)
library(tidytext)
library(glue)
library(stringr)
library(stringi)
library(rvest)
library(readr)
library(ptstem)
library(wordcloud2)
```
##Realiza o login no twitter
Realizando o login via Oauth no twitter. Os valores dos parametros devem ser recuperados via [Twitter](http://apps.twitter.com)

```{r}
setup_twitter_oauth(consumer_key = "CONSUMER KEY",
                    consumer_secret = "consumer_secret",
                    access_token = "access_token",
                    access_secret = "access_secret")


```
Carrega stop words do idioma Pt-BR, encoding UTF-8
```{r}

stopwordsPage <- read_html("https://www.ranks.nl/stopwords/brazilian", enconding="UTF-8")
stopwordsPage
stopwordsList <- html_nodes(stopwordsPage,'td') 


```
Limpa o HTML, alterando para TAGS
```{r}
xml_find_all(stopwordsList, ".//br") %>% xml_add_sibling("p", "")
xml_find_all(stopwordsList, ".//br") %>% xml_remove()

swstr <- html_text(stopwordsList)
swstr


```
Trata os dados para torna-los dicionarios
```{r}

sw <- unlist(str_split(swstr,'\\n')) 
glimpse(sw)
sw
library(tm)
```
Carrega stopword da TM, uma ótima biblioeta para mineração de dados em PT.
```{r}

swList2 <- stopwords('portuguese')
glimpse(swList2)

```
Realiza o merge usando a biblioteca do tidyverse, extremamento útil para o projeto.
```{r}
str(sw)

sw_merged <- union(sw,swList2) 
summary(sw_merged)

#Verifica se temos termos repetidos
tibble(word = sw_merged) %>% 
  group_by(word) %>% 
  filter(n()>1)

```
##Carregando termos positivos e negativos
Neste momento, foi realizado uma carga em tabela dos arquivos da [Linguateca](http://www.linguateca.pt/Repositorio/ReLi/). Os arquivos foram organizados e salvados no mesmo repositorio de trabalho para facilitar a leitura.
Dica:É interessante verificar se os arquivos foram salvos em UTF-8, pois mais a frente enfrentei problemas e tive de voltar e verificar.
```{r}

#Verificar se os arquivos estão em UTF-8 no notepad++ por ex
an <- read.csv("adjetivos_negativos.txt", header = F, sep = "\t", strip.white = F,
               stringsAsFactors = F, encoding="UTF-8")
exn <- read.csv("expressoes_negativas.txt", header = F, sep = "\t", strip.white = F,
                stringsAsFactors = F, encoding="UTF-8")

vn <- read.csv("verbos_negativos.txt", header = F, sep = "\t", strip.white = F, 
               stringsAsFactors = F, encoding="UTF-8")

subn <- read.csv("substantivos_negativos.txt", header = F, sep = "\t", strip.white = F, 
                 stringsAsFactors = F, encoding="UTF-8")

ap <- read.csv("adjetivos_positivos.txt", header = F, sep = "\t", strip.white = F, 
               stringsAsFactors = F, encoding="UTF-8")

exp <- read.csv("expressoes_positivas.txt", header = F, sep = "\t", strip.white = F, 
                stringsAsFactors = F, encoding="UTF-8")

vp <- read.csv("verbos_positivos.txt", header = F, sep = "\t", strip.white = F, 
               stringsAsFactors = F, encoding="UTF-8")

sp <- read.csv("substantivos_positivos.txt", header = F, sep = "\t", strip.white = F, 
               stringsAsFactors = F, encoding="UTF-8")


str(an);str(exn)

```
##Carregando termos do Kaggle
No Kaggle também existe um CSV com palavras positivas e negativas. Encontradas [aqui](https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages/data)
```{r}

poskaggle <- read.csv("positive_words_pt.txt", header = F, sep = "\t", strip.white = F, 
                      stringsAsFactors = F, encoding="UTF-8")

negkaggle <- read.csv("negative_words_pt.txt", header = F, sep = "\t", strip.white = F, 
                      stringsAsFactors = F, encoding="UTF-8")

#imprime para verificação
head(negkaggle)

```
Criando um dataframe para guardar as polaridades
```{r}

dfPolaridades <- an %>% 
  mutate(word = V1, polaridade = -1, tipo='adjetivo', sentimento='negativo') %>%
  select(word,polaridade,tipo,sentimento) %>%
  arrange(word)
head(dfPolaridades,2)

#Realiza um count para facilitar a inclusao dos dados
pcount <-  length(exn$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = exn$V1, polaridade=rep(-1,pcount),tipo=rep('expressao',pcount),sentimento=rep('negativo',pcount)))
dfPolaridades %>% arrange(desc(word)) %>% head(3)

pcount <-  length(vn$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = vn$V1, polaridade=rep(-1,pcount),tipo=rep('verbo',pcount),sentimento=rep('negativo',pcount)))

pcount <-  length(subn$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = subn$V1, polaridade=rep(-1,pcount),tipo=rep('substantivo',pcount),sentimento=rep('negativo',pcount)))

pcount <-  length(negkaggle$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = negkaggle$V1, polaridade=rep(-1,pcount),tipo=rep('noclass',pcount),sentimento=rep('negativo',pcount)))

pcount <-  length(ap$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = ap$V1, polaridade=rep(1,pcount),tipo=rep('adjetivo',pcount),sentimento=rep('positivo',pcount)))

pcount <-  length(exp$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = exp$V1, polaridade=rep(1,pcount),tipo=rep('expressao',pcount),sentimento=rep('positivo',pcount)))

pcount <-  length(vp$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = vp$V1, polaridade=rep(1,pcount),tipo=rep('verbo',pcount),sentimento=rep('positivo',pcount)))

pcount <-  length(sp$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = sp$V1, polaridade=rep(1,pcount),tipo=rep('substantivo',pcount),sentimento=rep('positivo',pcount)))

pcount <-  length(poskaggle$V1)
dfPolaridades <- bind_rows(dfPolaridades,list(word = poskaggle$V1, polaridade=rep(1,pcount),tipo=rep('noclass',pcount),sentimento=rep('positivo',pcount)))

#Visualiza o dataframe
dfPolaridades %>% group_by(word) %>% filter(n() == 1) %>% summarize(n=n())

dfPolaridades %>% count()

#Remove duplicados
dfPolaridadesUnique <- dfPolaridades[!duplicated(dfPolaridades$word),]
dfPolaridadesUnique %>% count()


```
##Realizando a busca de tweets
Realiza a busca de tweets. O primeiro parametro é a keyword de busca, e o seugndo é a quantidade de tweets a serem buscados.

```{r}

tweets <- searchTwitter("sicredi", n = 300)

#Pega os textos dos tweets
tweetxt <- sapply(tweets, function(x) x$getText())

#Vizualizando os dados
tibble(tweetxt)

```
Realiza uma limpeza dos tweets, retirando @ links e etcs
```{r}

tweetxtUtf <- NULL
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
tweetxtUtf <-  readr::parse_character(tweetxt, locale = readr::locale('pt'))
tweetxtUtf <- sapply(tweetxtUtf, function(x) iconv(x, "UTF-8"))
tweetxtUtf <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ",  tweetxtUtf);
tweetxtUtf <- str_replace(tweetxtUtf,"RT @[a-z,A-Z]*: ","")
tweetxtUtf <- gsub("@\\w+", "", tweetxtUtf)
tweetxtUtf <- removeURL(tweetxtUtf)
tweetxtUtf <- str_replace_all(tweetxtUtf,"@[a-z,A-Z]*","")  
tweetxtUtf <- gsub("[^[:alnum:][:blank:]!?]", " ", tweetxtUtf)
tweetxtUtf <- gsub("[[:digit:]]", "", tweetxtUtf)

#Verifica como estão os dados
tibble(tweetxtUtf)

```
É interessante remover os tweets duplicados, conforme abaixo
```{r}
length(tweetxtUtf)

tibble(tweetxtUtf) %>% unique() %>% count()

tweetxtUtfUnique <- tweetxtUtf %>% unique() 
length(tweetxtUtfUnique)

```
Removendo as stopwords
```{r}
#Removendo stopwords
tweetxtUtfUniqueSw <- tm::removeWords(tweetxtUtfUnique,c(sw_merged,'rt'))
tibble(tweetxtUtfUniqueSw)

ttokens <- data_frame(word= tweetxtUtfUniqueSw) %>% unnest_tokens(word,word)
ttokens %>% count(word, sort = T) 

ttokens_filter <- ttokens %>% filter(nchar(word) > 3)
ttokens_filter %>% count(word, sort=T)

ttokens_freq <- ttokens_filter %>% count(word, sort = T) %>% select(word, freq=n) 
ttokens_freq


```
##Nuvem de palavras
Agora podemos montar uma nuvem de palavras utilizando o Wordcloud2. Com isso teremos as palavras mais utilizadas maiores e a menos utilizadas menores etc.
Incluí varios tipos de núvens para exemplificar.
```{r}
require(devtools)
library(wordcloud2)
#Tive problemas e foi necessario instalar a partir do repositorio
#FicaDica
#install_github("lchiffon/wordcloud2")
figPath = system.file("examples/t.png",package = "wordcloud2")

#nuvem com imagem twitter
wordcloud2(ttokens_freq, figPath = figPath,minSize = 2, size = 1.5,color = "skyblue")

#nuvem tradicional
wordcloud2(ttokens_freq , minSize = 2, size = 3, backgroundColor = 'gray')

#rotacao
wordcloud2(ttokens_freq, minRotation = -pi/6, maxRotation = -pi/6, minSize = 10,
           rotateRatio = 1)

#outro tipo
wordcloud2(ttokens_freq, color = "random-light", backgroundColor = "grey", minSize = 2,size = 1)

```
Resolvi salvar os datasets para continuar o trabalho posteriormente sem perder nada.
```{r}
write_csv(tibble(word = sw_merged),path = 'stopwords.csv')
write_csv(as.tibble(dfPolaridadesUnique),path = 'polaridades_pt.csv')
write_csv(tibble(tweet = tweetxt),path = 'tweetxt.csv')
write_csv(tibble(tweet = tweetxtUtfUniqueSw),path='tweets_limpo.csv')
write_csv(ttokens_freq, path='tokens.csv')
```
Carregando as polaridades (não necessario para execucao linear do codigo)
```{r}
polaridades_pt <- read_csv('polaridades_pt.csv')
stopwordslist <- read_csv('stopwords.csv')
stopwordslist
```
É imprescindivel utilizar a funcao enc2native, pois do contrario o  TM vai destruír seu dataset.
```{r}
tweetencoded <- sapply(tweetxtUtfUnique,enc2native)

tibble(tweetencoded)
# TIRA VALORES NULOS - Caso fique nulos, teremos erro logo a frente
dd <- tweetencoded %>% na.omit()
##drop_na(tweetencoded)
tibble(dd)
df <- data.frame(text=dd)
head(df)
```
Cria um id para o documento e verifica os dados. Logo após, criamos o corpus e o DTM com a frequencia desejada. Com isso, verificamos os termos mais frequentes do nosso dataset.
```{r}

df$doc_id <- row.names(df)
head(df)

tm_corpus <- Corpus(DataframeSource(df))
inspect(tm_corpus[1:10])
dtm <- DocumentTermMatrix(tm_corpus, control=list(wordLengths=c(4, 20), language=locale('pt'), stopwords=stopwords('portuguese')))

dtm <- DocumentTermMatrix(tm_corpus, control=list(wordLengths=c(4,20),
                                                  language=locale('pt'), 
                                                  stopwords=stopwords('portuguese'),
                                                  bounds = list(global = c(3,500))))

dtm

#termos mais frequentes
findFreqTerms(dtm)

ttm_results <- t(as.matrix(dtm)) %*% as.matrix(dtm)
head(ttm_results)

```

Agora, vamos contruir o gráfico e os degrees dos vertices para vizualiação dos dados.
```{r}

library(igraph)
g <- graph.adjacency(ttm_results, weighted=T, mode = 'undirected')
g <- simplify(g)
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
E(g)$color <- ifelse(E(g)$wheight > 15, "lightblue", "red")
set.seed(2000)
par(bg = 'white')
layout1 <- layout_on_sphere(g)
plot(g, layout=layout.fruchterman.reingold)
```

Criando um tibble com labels das qualificações e verificar as quantidade de termos em gráfico
```{r}

library(quanteda)
twdf <- tibble(tweet = tweetxtUtfUnique)
twdf$whois <- NA
twdf$whois[twdf$tweet %like% 'sicredi'] <- 'sicredi'
twdf$whois[twdf$tweet %like% 'agência'] <- 'agencia'
twdf$whois[twdf$tweet %like% 'gerente'] <- 'gerente'
twdf$whois[twdf$tweet %like% 'negócios'] <- 'negocios'
twdf$whois[twdf$tweet %like% 'banco'] <- 'banco'
twdf$whois[twdf$tweet %like% 'simples'] <- 'simples'
twdf$whois[twdf$tweet %like% 'melhor'] <- 'melhor'
twdf$whois[twdf$tweet %like% 'juntos'] <- 'juntos'
# cria uma qualificação sem_tag como vala comum no dataset
twdf$whois[is.na(twdf$whois) ] <- 'semtag'
freq <- twdf %>% count(whois, sort = T) %>% select( whois,freq = n) 
freq

#plotando essa loucura
pie(table(twdf$whois))

#grafico em barras
barplot(table(twdf$whois))

#verificando a densidade dos termos
distMatrix <- as.matrix(dist(freq$freq))
plot(density(distMatrix))
```
##Matriz de termos por documento
Esta é uma parte importante na analise. Vamos criar a matriz de termos por documento para analisar os tweets. Utilizaremos a library quanteda, que tambem é utilizada para tokenizar os textos. Ele tambem converte os dados e realiza uma limpeza, como retirar pontuação, converte tudo para minusculo dentre outras funções.
```{r}

dfq <- data.frame(id=row.names(twdf),
                  text=twdf$tweet, whois = factor(twdf$whois))

myCorpus <- corpus(twdf,  text_field = 'tweet', 
                   metacorpus = list(source = "tweets sobre o sicredi")) 
myCorpus

#validando legibilidade
head(textstat_readability(myCorpus),2)

#validando
summary(myCorpus,6)

#tokenizando o texto
temptok <- tokens(tweetxtUtfUnique)
#verificando
temptok[1:5]

remove(temptok)

#utilizando o quanteda
myDfm <- dfm(myCorpus, stem = F)
myDfm
topfeatures(myDfm,20)

stopwors2 <- c('the','r','é','c','?','!','of','rt','pra')
myDfm <- dfm(myCorpus, groups='whois', remove = c(quanteda::stopwords("portuguese"),stopwors2,tm::stopwords('portuguese')), 
             stem = F, remove_punct = TRUE)
#ele agrupa pela qualificação
myDfm

```
Vamos verificar a frequencia no texto
```{r}

#frequencia do texto
allfeats <- textstat_frequency(myDfm)
allfeats$feature <- with(allfeats, reorder(feature, -frequency))
#vizualizando os termos por frequencia
ggplot(head(allfeats,20), aes(x=feature, y=frequency, fill=frequency)) + geom_bar(stat="identity") +
  xlab("Termos") + ylab("Frequência") + coord_flip() +
  theme(axis.text=element_text(size=7))

#termos mais frequentes do dataset
col <- textstat_collocations(myCorpus , size = 2:4, min_count = 2)
head(col)

topfeatures(myDfm, 20) 

set.seed(100)

```
##Realizando a vizualicação dos termos mais frequentes!
```{r}
#plotando os termos!
textplot_wordcloud(myDfm, min.freq = 15, random.order = FALSE,
                   rot.per = .6, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))

#grafico por cores
ggplot(col[order(col$count, decreasing = T),][1:25,], 
       aes(x=reorder(collocation,count), y=factor(count), fill=factor(count))) + geom_bar(stat="identity") +
  xlab("Expressões") + ylab("Frequência")  + coord_flip() +
  theme(axis.text=element_text(size=7))
```


Vamos vizualizar a relação entre as tags encontradas no dataset
```{r}

##########################
twraw <- readr::parse_character(tweetxt, locale = readr::locale('pt')) 
mytoken <- tokens(twraw, 
                  remove_numbers=T,remove_symbols=T, 
                  remove_twitter=T, remove_url=T)
head(mytoken)

##
mytoken <- tokens_remove(mytoken, stopwords('portuguese'))
head(textstat_collocations(mytoken,size = 5, min_count = 5))


#
myrawCorpus <- corpus(twraw)
tweetdfm <- dfm(myrawCorpus, remove_punct = TRUE)
tagdfm <- dfm_select(tweetdfm, ('#*'))
toptag <- names(topfeatures(tagdfm, 50))
head(toptag)
#
tagfcm <- fcm(tagdfm)
head(tagfcm)

toptagfcm <- fcm_select(tagfcm, toptag)
textplot_network(toptagfcm, min_freq = 0.1, edge_alpha = 0.8, edge_size = 5)


```
Vizualizando a relação entre os usuarios que originaram o dataset
```{r}

## criando um grafico de relação de usuarios
userdfm <- dfm_select(tweetdfm, ('@*'))
topuser <- names(topfeatures(userdfm, 200))
userfcm <- fcm(userdfm)
userfcm <- fcm_select(userfcm, topuser)
textplot_network(userfcm, min_freq = 0.1, edge_color = 'blue', edge_alpha = 0.8, edge_size = 5)

#criando o dtm com as polaridade positivas e negativas
positivas <- polaridades_pt %>% filter(sentimento == 'positivo') %>% select(word)
negativas <- polaridades_pt %>% filter(sentimento == 'negativo') %>% select(word)

dic <- dictionary(list(positivas=as.character(positivas$word), negativas=as.character(negativas$word)))
bySentimento <- dfm(myCorpus, dictionary = dic)
library(tidytext)
scorebygroup <- tidy(bySentimento %>% 
                       dfm_group(groups='whois') )
scorebygroup

```

##Analise de sentimento
Vamos realizar agora a análise de sentimento do nosso dataset.


```{r}
#salvando o dataframe
twdf %>% write_csv(path='twittersentimentaldata.csv')
twdf <- read_csv('twittersentimentaldata.csv')
twdf$id <- rownames(twdf)
tw <- twdf %>% mutate(document = id,word=tweet) %>% select(document,word,whois)
#
str(tw)

tdm <- tw %>% unnest_tokens(word,word) 
#removendo stop words carregadas
tdm <- tdm %>% anti_join(data.frame(word= stopwords('portuguese')))
tdm <- tdm %>% anti_join(data.frame(word= stopwors2))
head(tdm)

```

##Realizando a vizualição das primeiras impressões da análise
Vamos plotar os dados, e em seguida ajustar os dados para vizualiar a porcentagem e as palavras mais utilizadas por sentimento.
```{r}

library(tidyr)
sentJoin <- tdm %>%
  inner_join(polaridades_pt, by='word')

sentJoin %>%
  count(sentimento) %>%
  ggplot(aes(sentimento,n , fill = sentimento)) +
  geom_bar(stat = "identity", show.legend = FALSE)

###
sentJoin %>%
  count(whois, index = document, sentimento) %>%
  spread(sentimento, n, fill = 0) %>%
  mutate(score = positivo - negativo) %>%
  ggplot(aes(index, score, fill = whois)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~whois, ncol = 2, scales = "free_x")

##ajusta para buscar a porcentagem
scored <- sentJoin %>%
  count(whois,sentimento) %>%
  spread(sentimento, n, fill = 0) %>%
  mutate(score = positivo -negativo) %>%
  mutate(scoreperc = (positivo / (positivo + negativo)) * 100)
#plota
ggplot(scored, aes(whois,scoreperc , fill = whois)) +
  geom_bar(stat = "identity", show.legend = T) 


#palavras por sentimento
word_counts <- sentJoin %>%
  count(word, sentimento, sort = TRUE) %>%
  ungroup()

word_counts %>%
  filter(n > 5) %>%
  mutate(n = ifelse(sentimento == "negativo", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentimento)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribuição para o sentimento")
```

##Nuvem de sentimento
Vamos criar uma nuvem de sentimento com os dados tratados.
Criaremos uma nuvem por sentimento positivo e negativo para analisarmos.
```{r}

####
#### nuvem de sentimentos
####
library(reshape2)
library(wordcloud)
sentJoin %>%
  count(word, sentimento, sort = TRUE) %>%
  acast(word ~ sentimento, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 60)

## extra
# sentimentos mais negativos
bottom8tw <- head(sentJoin %>%
                    count(document, sentimento) %>%
                    spread(sentimento, n, fill = 0) %>%
                    mutate(score = positivo - negativo) %>%
                    arrange(score),8)['document']


twdf %>% filter(id %in% as.vector(bottom8tw$document))

#positivos
top8 <- head(sentJoin %>%
               count(document, sentimento) %>%
               spread(sentimento, n, fill = 0) %>%
               mutate(score = positivo - negativo) %>%
               arrange(desc(score)),8)['document']
twdf %>% filter(id %in% as.vector(top8$document))


```

##analise de sentimento usando sentR
Tambem usei a biblioteca sentR para analisar os sentimento dos tweets. Esta biblioeta utiliza Naive Bayes para a classificação. Vamos verificar como o algoritmo classifica os tweets.
```{r}

#install_github('mananshah99/sentR')
require(sentR)

sentimentToScore <-  sample_n(data.frame(text=twdf$tweet),100)
#Aplicando o metodo de classificação Naive Bayes
out <- classify.naivebayes(sentimentToScore$text)
scoredDf <- cbind(sentimentToScore,out, stringsAsFactors=F)
scoredDf$`POS/NEG` <- as.numeric(scoredDf$`POS/NEG`) 
s <-head(scoredDf %>% arrange(`POS/NEG`) %>% select(text),10) 
#negativas
s[,1]

#mais positivas
s<-head(scoredDf %>% arrange(desc(`POS/NEG`)) %>% select(text),10) 

s[,1]
```
##Outra biblioteca: sentimentAnalisys
Esta biblioteca tambem realiza a analise de sentimento. Vamos verificar como ficou o resultado.
```{r}

##
#pacote sentimentAnalisys
install.packages("SentimentAnalysis")
library(SentimentAnalysis)
dictionaryPortuguese <- SentimentDictionaryBinary(positivas$word, 
                                                  negativas$word)
twdf$id <- row.names(twdf)
sentiment <- analyzeSentiment(twdf$tweet,
                              language="portuguese",
                              rules=list("PtSentiment"=list(ruleSentiment, dictionaryPortuguese), 
                                         "Ratio"=list(ruleSentimentPolarity,dictionaryPortuguese),
                                         "Words"=list(ruleWordCount)))
#sentiment
plotSentiment(sentiment)

```
##Conclusão
Por se tratar de uma empresa do ramo fincanceiro, muitas vezes realizando tarefas de banco, grande parte da motivaçao dos tweets são para reclamação. Com isso, deve-se considerar que boa parte dos tweets sejam negativos, embora tenhamos evidenciado tweets positivos a respeito de campanhas de marketing e de promoções realizad pela empresa.

A linguagem R se mostrou muito poderosa para vizualiação dos dados. Comparando ao python, demonstrou ser mais eficiente.
A análise de sentimento em Pt-Br ainda é complicada e isso deve ser considerado. Ainda é um nicho a ser explorado pela comunidade e não temos muitas bibliotecas alimentadas com o portugues, e por vezes foi verificado uma análise incorreta do sentimento do tweet.

